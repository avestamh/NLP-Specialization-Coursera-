{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0244b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from os import getcwd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dabf2dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/sadra/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/sadra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943a5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n",
    "# this enables importing of these files without downloading it again when we refresh our workspace\n",
    "\n",
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d80b48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc607af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Processing Tweets\n",
    "def process_tweets(tweet):\n",
    "    ## initiat stemming class:\n",
    "    stemmer = PorterStemmer()\n",
    "    ## remove old style retwee text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]', '', tweet)\n",
    "    ## remove  hyper link\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "    ### only rmove the # sighn from the word\n",
    "    tweet = re.sub(r'#','', tweet)\n",
    "    ## instantiate tokenizer class\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    ## Tokenize tweets\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    ## import English stop words list form NLTK\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # creat a list of words without stopwords\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "               word not in string.punctuation): ## remove punctuation\n",
    "            stem_word = stemmer.stem(word)  ## stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "#     print('tweets_clean: ', tweets_clean)\n",
    "    return tweets_clean\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1ce392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency generation function\n",
    "def build_freqs(tweets, ys):\n",
    "\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "#     print('ys: ', ys) \n",
    "    \n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "  \n",
    "    ylist = np.squeeze(ys).tolist()\n",
    "#     print('ylist: ', ylist)\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(ylist, tweets):\n",
    "        for word in process_tweets(tweet):\n",
    "            pair = (word,y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] =+ 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fac64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "    # calculate the sigmoid of z\n",
    "    h = 1/(1+np.exp(-z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea66af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x,y, theta, alpha, num_iters):\n",
    "    \n",
    "    '''Input:\n",
    "        x: matrix of feature which is (m, n+1)\n",
    "        y: corresponding label of the input matrix x, dimmensions (m,1)\n",
    "        theta: weigh vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of  iterations you to train your model for\n",
    "        \n",
    "        Output:\n",
    "        J: the final cost\n",
    "        theta: your final  weigh vector\n",
    "        Hint: you might want to print cost to make sure that it is going down\n",
    "    '''\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0,num_iters):\n",
    "        # get z, the dot producto of x and theta\n",
    "        z = np.dot(x, theta)\n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the function\n",
    "        J = (-1/m)*(np.dot(y.T, np.log(h)) + np.dot((1-y).T, np.log(1-h)))\n",
    "        \n",
    "        # update the weighs theta\n",
    "        theta = theta -(alpha/m)*np.dot(x.T, h-y)\n",
    "        J = float(J)\n",
    "        return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f20b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input:\n",
    "    tweet a list of words for one tweet\n",
    "    freqs: dictionary corresponding to the frequencies of each tuple(word,label)\n",
    "    \n",
    "    Output:\n",
    "    x: a feature vector of dimmension (1,3)\n",
    "    '''\n",
    "    # process_tweet, tokenizes, stems, and remove stopwords\n",
    "    word_l = process_tweets(tweet)\n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1,3))\n",
    "    # bisa term is set to 1\n",
    "    x[0,0] = 1\n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        # increment the word count for the positive label 1\n",
    "        if freqs.get((word, 1.0))!=None:\n",
    "            x[0,1] += freqs.get((word, 1.0))\n",
    "        \n",
    "        #increment the word count for label 0\n",
    "        if freqs.get((word, 0.0))!=None:\n",
    "            x[0,2] += freqs.get((word, 0.0))\n",
    "    \n",
    "    assert(x.shape==(1,3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac25b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data into two pieces, one for training and for testing (validation set)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos),1)), np.zeros((len(train_neg),1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg),1)), axis=0)\n",
    "# combine positive and negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "099060e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d98acea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('followfriday', 1.0): 1,\n",
       " ('top', 1.0): 1,\n",
       " ('engag', 1.0): 1,\n",
       " ('member', 1.0): 1,\n",
       " ('commun', 1.0): 1,\n",
       " ('week', 1.0): 1,\n",
       " (':)', 1.0): 1,\n",
       " ('hey', 1.0): 1,\n",
       " ('jame', 1.0): 1,\n",
       " ('odd', 1.0): 1,\n",
       " (':/', 1.0): 1,\n",
       " ('pleas', 1.0): 1,\n",
       " ('call', 1.0): 1,\n",
       " ('contact', 1.0): 1,\n",
       " ('centr', 1.0): 1,\n",
       " ('02392441234', 1.0): 1,\n",
       " ('abl', 1.0): 1,\n",
       " ('assist', 1.0): 1,\n",
       " ('mani', 1.0): 1,\n",
       " ('thank', 1.0): 1,\n",
       " ('listen', 1.0): 1,\n",
       " ('last', 1.0): 1,\n",
       " ('night', 1.0): 1,\n",
       " ('bleed', 1.0): 1,\n",
       " ('amaz', 1.0): 1,\n",
       " ('track', 1.0): 1,\n",
       " ('scotland', 1.0): 1,\n",
       " ('congrat', 1.0): 1,\n",
       " ('yeaaah', 1.0): 1,\n",
       " ('yipppi', 1.0): 1,\n",
       " ('accnt', 1.0): 1,\n",
       " ('verifi', 1.0): 1,\n",
       " ('rqst', 1.0): 1,\n",
       " ('succeed', 1.0): 1,\n",
       " ('got', 1.0): 1,\n",
       " ('blue', 1.0): 1,\n",
       " ('tick', 1.0): 1,\n",
       " ('mark', 1.0): 1,\n",
       " ('fb', 1.0): 1,\n",
       " ('profil', 1.0): 1,\n",
       " ('15', 1.0): 1,\n",
       " ('day', 1.0): 1,\n",
       " ('one', 1.0): 1,\n",
       " ('irresist', 1.0): 1,\n",
       " ('flipkartfashionfriday', 1.0): 1,\n",
       " ('like', 1.0): 1,\n",
       " ('keep', 1.0): 1,\n",
       " ('love', 1.0): 1,\n",
       " ('custom', 1.0): 1,\n",
       " ('wait', 1.0): 1,\n",
       " ('long', 1.0): 1,\n",
       " ('hope', 1.0): 1,\n",
       " ('enjoy', 1.0): 1,\n",
       " ('happi', 1.0): 1,\n",
       " ('friday', 1.0): 1,\n",
       " ('lwwf', 1.0): 1,\n",
       " ('second', 1.0): 1,\n",
       " ('thought', 1.0): 1,\n",
       " ('’', 1.0): 1,\n",
       " ('enough', 1.0): 1,\n",
       " ('time', 1.0): 1,\n",
       " ('dd', 1.0): 1,\n",
       " ('new', 1.0): 1,\n",
       " ('short', 1.0): 1,\n",
       " ('enter', 1.0): 1,\n",
       " ('system', 1.0): 1,\n",
       " ('sheep', 1.0): 1,\n",
       " ('must', 1.0): 1,\n",
       " ('buy', 1.0): 1,\n",
       " ('jgh', 1.0): 1,\n",
       " ('go', 1.0): 1,\n",
       " ('bayan', 1.0): 1,\n",
       " (':d', 1.0): 1,\n",
       " ('bye', 1.0): 1,\n",
       " ('act', 1.0): 1,\n",
       " ('mischiev', 1.0): 1,\n",
       " ('etl', 1.0): 1,\n",
       " ('layer', 1.0): 1,\n",
       " ('in-hous', 1.0): 1,\n",
       " ('wareh', 1.0): 1,\n",
       " ('app', 1.0): 1,\n",
       " ('katamari', 1.0): 1,\n",
       " ('well', 1.0): 1,\n",
       " ('…', 1.0): 1,\n",
       " ('name', 1.0): 1,\n",
       " ('impli', 1.0): 1,\n",
       " (':p', 1.0): 1,\n",
       " ('influenc', 1.0): 1,\n",
       " ('big', 1.0): 1,\n",
       " ('...', 1.0): 1,\n",
       " ('juici', 1.0): 1,\n",
       " ('selfi', 1.0): 1,\n",
       " ('follow', 1.0): 1,\n",
       " ('u', 1.0): 1,\n",
       " ('back', 1.0): 1,\n",
       " ('perfect', 1.0): 1,\n",
       " ('alreadi', 1.0): 1,\n",
       " ('know', 1.0): 1,\n",
       " (\"what'\", 1.0): 1,\n",
       " ('great', 1.0): 1,\n",
       " ('opportun', 1.0): 1,\n",
       " ('junior', 1.0): 1,\n",
       " ('triathlet', 1.0): 1,\n",
       " ('age', 1.0): 1,\n",
       " ('12', 1.0): 1,\n",
       " ('13', 1.0): 1,\n",
       " ('gatorad', 1.0): 1,\n",
       " ('seri', 1.0): 1,\n",
       " ('get', 1.0): 1,\n",
       " ('entri', 1.0): 1,\n",
       " ('lay', 1.0): 1,\n",
       " ('greet', 1.0): 1,\n",
       " ('card', 1.0): 1,\n",
       " ('rang', 1.0): 1,\n",
       " ('print', 1.0): 1,\n",
       " ('today', 1.0): 1,\n",
       " ('job', 1.0): 1,\n",
       " (':-)', 1.0): 1,\n",
       " (\"friend'\", 1.0): 1,\n",
       " ('lunch', 1.0): 1,\n",
       " ('yummm', 1.0): 1,\n",
       " ('nostalgia', 1.0): 1,\n",
       " ('tb', 1.0): 1,\n",
       " ('ku', 1.0): 1,\n",
       " ('id', 1.0): 1,\n",
       " ('conflict', 1.0): 1,\n",
       " ('help', 1.0): 1,\n",
       " (\"here'\", 1.0): 1,\n",
       " ('screenshot', 1.0): 1,\n",
       " ('work', 1.0): 1,\n",
       " ('hi', 1.0): 1,\n",
       " ('liv', 1.0): 1,\n",
       " ('hello', 1.0): 1,\n",
       " ('need', 1.0): 1,\n",
       " ('someth', 1.0): 1,\n",
       " ('fm', 1.0): 1,\n",
       " ('twitter', 1.0): 1,\n",
       " ('—', 1.0): 1,\n",
       " ('sure', 1.0): 1,\n",
       " ('thing', 1.0): 1,\n",
       " ('dm', 1.0): 1,\n",
       " ('x', 1.0): 1,\n",
       " (\"i'v\", 1.0): 1,\n",
       " ('heard', 1.0): 1,\n",
       " ('four', 1.0): 1,\n",
       " ('season', 1.0): 1,\n",
       " ('pretti', 1.0): 1,\n",
       " ('dope', 1.0): 1,\n",
       " ('penthous', 1.0): 1,\n",
       " ('obv', 1.0): 1,\n",
       " ('gobigorgohom', 1.0): 1,\n",
       " ('fun', 1.0): 1,\n",
       " (\"y'all\", 1.0): 1,\n",
       " ('yeah', 1.0): 1,\n",
       " ('suppos', 1.0): 1,\n",
       " ('lol', 1.0): 1,\n",
       " ('chat', 1.0): 1,\n",
       " ('bit', 1.0): 1,\n",
       " ('youth', 1.0): 1,\n",
       " ('💅🏽', 1.0): 1,\n",
       " ('💋', 1.0): 1,\n",
       " ('seen', 1.0): 1,\n",
       " ('year', 1.0): 1,\n",
       " ('rest', 1.0): 1,\n",
       " ('goe', 1.0): 1,\n",
       " ('quickli', 1.0): 1,\n",
       " ('bed', 1.0): 1,\n",
       " ('music', 1.0): 1,\n",
       " ('fix', 1.0): 1,\n",
       " ('dream', 1.0): 1,\n",
       " ('spiritu', 1.0): 1,\n",
       " ('ritual', 1.0): 1,\n",
       " ('festiv', 1.0): 1,\n",
       " ('népal', 1.0): 1,\n",
       " ('begin', 1.0): 1,\n",
       " ('line-up', 1.0): 1,\n",
       " ('left', 1.0): 1,\n",
       " ('see', 1.0): 1,\n",
       " ('sarah', 1.0): 1,\n",
       " ('send', 1.0): 1,\n",
       " ('us', 1.0): 1,\n",
       " ('email', 1.0): 1,\n",
       " ('bitsy@bitdefender.com', 1.0): 1,\n",
       " (\"we'll\", 1.0): 1,\n",
       " ('asap', 1.0): 1,\n",
       " ('kik', 1.0): 1,\n",
       " ('hatessuc', 1.0): 1,\n",
       " ('32429', 1.0): 1,\n",
       " ('kikm', 1.0): 1,\n",
       " ('lgbt', 1.0): 1,\n",
       " ('tinder', 1.0): 1,\n",
       " ('nsfw', 1.0): 1,\n",
       " ('akua', 1.0): 1,\n",
       " ('cumshot', 1.0): 1,\n",
       " ('come', 1.0): 1,\n",
       " ('hous', 1.0): 1,\n",
       " ('nsn_supplement', 1.0): 1,\n",
       " ('effect', 1.0): 1,\n",
       " ('press', 1.0): 1,\n",
       " ('releas', 1.0): 1,\n",
       " ('distribut', 1.0): 1,\n",
       " ('result', 1.0): 1,\n",
       " ('link', 1.0): 1,\n",
       " ('remov', 1.0): 1,\n",
       " ('pressreleas', 1.0): 1,\n",
       " ('newsdistribut', 1.0): 1,\n",
       " ('bam', 1.0): 1,\n",
       " ('bestfriend', 1.0): 1,\n",
       " ('lot', 1.0): 1,\n",
       " ('warsaw', 1.0): 1,\n",
       " ('<3', 1.0): 1,\n",
       " ('x46', 1.0): 1,\n",
       " ('everyon', 1.0): 1,\n",
       " ('watch', 1.0): 1,\n",
       " ('documentari', 1.0): 1,\n",
       " ('earthl', 1.0): 1,\n",
       " ('youtub', 1.0): 1,\n",
       " ('support', 1.0): 1,\n",
       " ('buuut', 1.0): 1,\n",
       " ('oh', 1.0): 1,\n",
       " ('look', 1.0): 1,\n",
       " ('forward', 1.0): 1,\n",
       " ('visit', 1.0): 1,\n",
       " ('next', 1.0): 1,\n",
       " ('letsgetmessi', 1.0): 1,\n",
       " ('jo', 1.0): 1,\n",
       " ('make', 1.0): 1,\n",
       " ('feel', 1.0): 1,\n",
       " ('better', 1.0): 1,\n",
       " ('never', 1.0): 1,\n",
       " ('anyon', 1.0): 1,\n",
       " ('kpop', 1.0): 1,\n",
       " ('flesh', 1.0): 1,\n",
       " ('good', 1.0): 1,\n",
       " ('girl', 1.0): 1,\n",
       " ('best', 1.0): 1,\n",
       " ('wish', 1.0): 1,\n",
       " ('reason', 1.0): 1,\n",
       " ('epic', 1.0): 1,\n",
       " ('soundtrack', 1.0): 1,\n",
       " ('shout', 1.0): 1,\n",
       " ('ad', 1.0): 1,\n",
       " ('video', 1.0): 1,\n",
       " ('playlist', 1.0): 1,\n",
       " ('im', 1.0): 1,\n",
       " ('twitch', 1.0): 1,\n",
       " ('leagu', 1.0): 1,\n",
       " ('1', 1.0): 1,\n",
       " ('4', 1.0): 1,\n",
       " ('would', 1.0): 1,\n",
       " ('dear', 1.0): 1,\n",
       " ('jordan', 1.0): 1,\n",
       " ('okay', 1.0): 1,\n",
       " ('fake', 1.0): 1,\n",
       " ('gameplay', 1.0): 1,\n",
       " (';)', 1.0): 1,\n",
       " ('haha', 1.0): 1,\n",
       " ('kid', 1.0): 1,\n",
       " ('stuff', 1.0): 1,\n",
       " ('exactli', 1.0): 1,\n",
       " ('product', 1.0): 1,\n",
       " ('line', 1.0): 1,\n",
       " ('etsi', 1.0): 1,\n",
       " ('shop', 1.0): 1,\n",
       " ('check', 1.0): 1,\n",
       " ('boxroomcraft', 1.0): 1,\n",
       " ('vacat', 1.0): 1,\n",
       " ('recharg', 1.0): 1,\n",
       " ('normal', 1.0): 1,\n",
       " ('charger', 1.0): 1,\n",
       " ('asleep', 1.0): 1,\n",
       " ('talk', 1.0): 1,\n",
       " ('sooo', 1.0): 1,\n",
       " ('someon', 1.0): 1,\n",
       " ('text', 1.0): 1,\n",
       " ('ye', 1.0): 1,\n",
       " ('bet', 1.0): 1,\n",
       " (\"he'll\", 1.0): 1,\n",
       " ('fit', 1.0): 1,\n",
       " ('hear', 1.0): 1,\n",
       " ('speech', 1.0): 1,\n",
       " ('piti', 1.0): 1,\n",
       " ('green', 1.0): 1,\n",
       " ('garden', 1.0): 1,\n",
       " ('midnight', 1.0): 1,\n",
       " ('sun', 1.0): 1,\n",
       " ('beauti', 1.0): 1,\n",
       " ('canal', 1.0): 1,\n",
       " ('dasvidaniya', 1.0): 1,\n",
       " ('till', 1.0): 1,\n",
       " ('scout', 1.0): 1,\n",
       " ('sg', 1.0): 1,\n",
       " ('futur', 1.0): 1,\n",
       " ('wlan', 1.0): 1,\n",
       " ('pro', 1.0): 1,\n",
       " ('confer', 1.0): 1,\n",
       " ('asia', 1.0): 1,\n",
       " ('chang', 1.0): 1,\n",
       " ('lollipop', 1.0): 1,\n",
       " ('🍭', 1.0): 1,\n",
       " ('nez', 1.0): 1,\n",
       " ('agnezmo', 1.0): 1,\n",
       " ('oley', 1.0): 1,\n",
       " ('mama', 1.0): 1,\n",
       " ('stand', 1.0): 1,\n",
       " ('stronger', 1.0): 1,\n",
       " ('god', 1.0): 1,\n",
       " ('misti', 1.0): 1,\n",
       " ('babi', 1.0): 1,\n",
       " ('cute', 1.0): 1,\n",
       " ('woohoo', 1.0): 1,\n",
       " (\"can't\", 1.0): 1,\n",
       " ('sign', 1.0): 1,\n",
       " ('yet', 1.0): 1,\n",
       " ('still', 1.0): 1,\n",
       " ('think', 1.0): 1,\n",
       " ('mka', 1.0): 1,\n",
       " ('liam', 1.0): 1,\n",
       " ('access', 1.0): 1,\n",
       " ('welcom', 1.0): 1,\n",
       " ('stat', 1.0): 1,\n",
       " ('arriv', 1.0): 1,\n",
       " ('unfollow', 1.0): 1,\n",
       " ('via', 1.0): 1,\n",
       " ('surpris', 1.0): 1,\n",
       " ('figur', 1.0): 1,\n",
       " ('happybirthdayemilybett', 1.0): 1,\n",
       " ('sweet', 1.0): 1,\n",
       " ('talent', 1.0): 1,\n",
       " ('2', 1.0): 1,\n",
       " ('plan', 1.0): 1,\n",
       " ('drain', 1.0): 1,\n",
       " ('gotta', 1.0): 1,\n",
       " ('timezon', 1.0): 1,\n",
       " ('parent', 1.0): 1,\n",
       " ('proud', 1.0): 1,\n",
       " ('least', 1.0): 1,\n",
       " ('mayb', 1.0): 1,\n",
       " ('sometim', 1.0): 1,\n",
       " ('grade', 1.0): 1,\n",
       " ('al', 1.0): 1,\n",
       " ('grand', 1.0): 1,\n",
       " ('manila_bro', 1.0): 1,\n",
       " ('chosen', 1.0): 1,\n",
       " ('let', 1.0): 1,\n",
       " ('around', 1.0): 1,\n",
       " ('..', 1.0): 1,\n",
       " ('side', 1.0): 1,\n",
       " ('world', 1.0): 1,\n",
       " ('eh', 1.0): 1,\n",
       " ('take', 1.0): 1,\n",
       " ('care', 1.0): 1,\n",
       " ('final', 1.0): 1,\n",
       " ('fuck', 1.0): 1,\n",
       " ('weekend', 1.0): 1,\n",
       " ('real', 1.0): 1,\n",
       " ('x45', 1.0): 1,\n",
       " ('join', 1.0): 1,\n",
       " ('hushedcallwithfraydo', 1.0): 1,\n",
       " ('gift', 1.0): 1,\n",
       " ('yeahhh', 1.0): 1,\n",
       " ('hushedpinwithsammi', 1.0): 1,\n",
       " ('event', 1.0): 1,\n",
       " ('might', 1.0): 1,\n",
       " ('luv', 1.0): 1,\n",
       " ('realli', 1.0): 1,\n",
       " ('appreci', 1.0): 1,\n",
       " ('share', 1.0): 1,\n",
       " ('wow', 1.0): 1,\n",
       " ('tom', 1.0): 1,\n",
       " ('3', 1.0): 1,\n",
       " ('gym', 1.0): 1,\n",
       " ('monday', 1.0): 1,\n",
       " ('invit', 1.0): 1,\n",
       " ('scope', 1.0): 1,\n",
       " ('friend', 1.0): 1,\n",
       " ('nude', 1.0): 1,\n",
       " ('sleep', 1.0): 1,\n",
       " ('birthday', 1.0): 1,\n",
       " ('want', 1.0): 1,\n",
       " ('t-shirt', 1.0): 1,\n",
       " ('cool', 1.0): 1,\n",
       " ('haw', 1.0): 1,\n",
       " ('phela', 1.0): 1,\n",
       " ('mom', 1.0): 1,\n",
       " ('obvious', 1.0): 1,\n",
       " ('princ', 1.0): 1,\n",
       " ('charm', 1.0): 1,\n",
       " ('stage', 1.0): 1,\n",
       " ('luck', 1.0): 1,\n",
       " ('tyler', 1.0): 1,\n",
       " ('hipster', 1.0): 1,\n",
       " ('glass', 1.0): 1,\n",
       " ('marti', 1.0): 1,\n",
       " ('glad', 1.0): 1,\n",
       " ('done', 1.0): 1,\n",
       " ('afternoon', 1.0): 1,\n",
       " ('read', 1.0): 1,\n",
       " ('kahfi', 1.0): 1,\n",
       " ('finish', 1.0): 1,\n",
       " ('ohmyg', 1.0): 1,\n",
       " ('yaya', 1.0): 1,\n",
       " ('dub', 1.0): 1,\n",
       " ('stalk', 1.0): 1,\n",
       " ('ig', 1.0): 1,\n",
       " ('gondooo', 1.0): 1,\n",
       " ('moo', 1.0): 1,\n",
       " ('tologooo', 1.0): 1,\n",
       " ('becom', 1.0): 1,\n",
       " ('detail', 1.0): 1,\n",
       " ('zzz', 1.0): 1,\n",
       " ('xx', 1.0): 1,\n",
       " ('physiotherapi', 1.0): 1,\n",
       " ('hashtag', 1.0): 1,\n",
       " ('💪', 1.0): 1,\n",
       " ('monica', 1.0): 1,\n",
       " ('miss', 1.0): 1,\n",
       " ('sound', 1.0): 1,\n",
       " ('morn', 1.0): 1,\n",
       " (\"that'\", 1.0): 1,\n",
       " ('x43', 1.0): 1,\n",
       " ('definit', 1.0): 1,\n",
       " ('tri', 1.0): 1,\n",
       " ('tonight', 1.0): 1,\n",
       " ('took', 1.0): 1,\n",
       " ('advic', 1.0): 1,\n",
       " ('treviso', 1.0): 1,\n",
       " ('concert', 1.0): 1,\n",
       " ('citi', 1.0): 1,\n",
       " ('countri', 1.0): 1,\n",
       " (\"i'll\", 1.0): 1,\n",
       " ('start', 1.0): 1,\n",
       " ('fine', 1.0): 1,\n",
       " ('gorgeou', 1.0): 1,\n",
       " ('xo', 1.0): 1,\n",
       " ('oven', 1.0): 1,\n",
       " ('roast', 1.0): 1,\n",
       " ('garlic', 1.0): 1,\n",
       " ('oliv', 1.0): 1,\n",
       " ('oil', 1.0): 1,\n",
       " ('dri', 1.0): 1,\n",
       " ('tomato', 1.0): 1,\n",
       " ('basil', 1.0): 1,\n",
       " ('centuri', 1.0): 1,\n",
       " ('tuna', 1.0): 1,\n",
       " ('right', 1.0): 1,\n",
       " ('atchya', 1.0): 1,\n",
       " ('even', 1.0): 1,\n",
       " ('almost', 1.0): 1,\n",
       " ('chanc', 1.0): 1,\n",
       " ('cheer', 1.0): 1,\n",
       " ('po', 1.0): 1,\n",
       " ('ice', 1.0): 1,\n",
       " ('cream', 1.0): 1,\n",
       " ('agre', 1.0): 1,\n",
       " ('100', 1.0): 1,\n",
       " ('heheheh', 1.0): 1,\n",
       " ('that', 1.0): 1,\n",
       " ('point', 1.0): 1,\n",
       " ('stay', 1.0): 1,\n",
       " ('home', 1.0): 1,\n",
       " ('soon', 1.0): 1,\n",
       " ('promis', 1.0): 1,\n",
       " ('web', 1.0): 1,\n",
       " ('whatsapp', 1.0): 1,\n",
       " ('volta', 1.0): 1,\n",
       " ('funcionar', 1.0): 1,\n",
       " ('com', 1.0): 1,\n",
       " ('iphon', 1.0): 1,\n",
       " ('jailbroken', 1.0): 1,\n",
       " ('later', 1.0): 1,\n",
       " ('34', 1.0): 1,\n",
       " ('min', 1.0): 1,\n",
       " ('leia', 1.0): 1,\n",
       " ('appear', 1.0): 1,\n",
       " ('hologram', 1.0): 1,\n",
       " ('r2d2', 1.0): 1,\n",
       " ('w', 1.0): 1,\n",
       " ('messag', 1.0): 1,\n",
       " ('obi', 1.0): 1,\n",
       " ('wan', 1.0): 1,\n",
       " ('sit', 1.0): 1,\n",
       " ('luke', 1.0): 1,\n",
       " ('inter', 1.0): 1,\n",
       " ('ucl', 1.0): 1,\n",
       " ('arsen', 1.0): 1,\n",
       " ('small', 1.0): 1,\n",
       " ('team', 1.0): 1,\n",
       " ('pass', 1.0): 1,\n",
       " ('🚂', 1.0): 1,\n",
       " ('dewsburi', 1.0): 1,\n",
       " ('railway', 1.0): 1,\n",
       " ('station', 1.0): 1,\n",
       " ('dew', 1.0): 1,\n",
       " ('west', 1.0): 1,\n",
       " ('yorkshir', 1.0): 1,\n",
       " ('430', 1.0): 1,\n",
       " ('smh', 1.0): 1,\n",
       " ('9:25', 1.0): 1,\n",
       " ('live', 1.0): 1,\n",
       " ('strang', 1.0): 1,\n",
       " ('imagin', 1.0): 1,\n",
       " ('megan', 1.0): 1,\n",
       " ('masaantoday', 1.0): 1,\n",
       " ('a4', 1.0): 1,\n",
       " ('shweta', 1.0): 1,\n",
       " ('tripathi', 1.0): 1,\n",
       " ('5', 1.0): 1,\n",
       " ('20', 1.0): 1,\n",
       " ('kurta', 1.0): 1,\n",
       " ('half', 1.0): 1,\n",
       " ('number', 1.0): 1,\n",
       " ('wsalelov', 1.0): 1,\n",
       " ('ah', 1.0): 1,\n",
       " ('larri', 1.0): 1,\n",
       " ('anyway', 1.0): 1,\n",
       " ('kinda', 1.0): 1,\n",
       " ('goood', 1.0): 1,\n",
       " ('life', 1.0): 1,\n",
       " ('enn', 1.0): 1,\n",
       " ('could', 1.0): 1,\n",
       " ('warmup', 1.0): 1,\n",
       " ('15th', 1.0): 1,\n",
       " ('bath', 1.0): 1,\n",
       " ('dum', 1.0): 1,\n",
       " ('andar', 1.0): 1,\n",
       " ('ram', 1.0): 1,\n",
       " ('sampath', 1.0): 1,\n",
       " ('sona', 1.0): 1,\n",
       " ('mohapatra', 1.0): 1,\n",
       " ('samantha', 1.0): 1,\n",
       " ('edward', 1.0): 1,\n",
       " ('mein', 1.0): 1,\n",
       " ('tulan', 1.0): 1,\n",
       " ('razi', 1.0): 1,\n",
       " ('wah', 1.0): 1,\n",
       " ('josh', 1.0): 1,\n",
       " ('alway', 1.0): 1,\n",
       " ('smile', 1.0): 1,\n",
       " ('pictur', 1.0): 1,\n",
       " ('16.20', 1.0): 1,\n",
       " ('giveitup', 1.0): 1,\n",
       " ('given', 1.0): 1,\n",
       " ('ga', 1.0): 1,\n",
       " ('subsidi', 1.0): 1,\n",
       " ('initi', 1.0): 1,\n",
       " ('propos', 1.0): 1,\n",
       " ('delight', 1.0): 1,\n",
       " ('yesterday', 1.0): 1,\n",
       " ('x42', 1.0): 1,\n",
       " ('lmaoo', 1.0): 1,\n",
       " ('song', 1.0): 1,\n",
       " ('ever', 1.0): 1,\n",
       " ('shall', 1.0): 1,\n",
       " ('littl', 1.0): 1,\n",
       " ('throwback', 1.0): 1,\n",
       " ('outli', 1.0): 1,\n",
       " ('island', 1.0): 1,\n",
       " ('cheung', 1.0): 1,\n",
       " ('chau', 1.0): 1,\n",
       " ('mui', 1.0): 1,\n",
       " ('wo', 1.0): 1,\n",
       " ('total', 1.0): 1,\n",
       " ('differ', 1.0): 1,\n",
       " ('kfckitchentour', 1.0): 1,\n",
       " ('kitchen', 1.0): 1,\n",
       " ('clean', 1.0): 1,\n",
       " (\"i'm\", 1.0): 1,\n",
       " ('cusp', 1.0): 1,\n",
       " ('test', 1.0): 1,\n",
       " ('water', 1.0): 1,\n",
       " ('reward', 1.0): 1,\n",
       " ('arummzz', 1.0): 1,\n",
       " (\"let'\", 1.0): 1,\n",
       " ('drive', 1.0): 1,\n",
       " ('travel', 1.0): 1,\n",
       " ('yogyakarta', 1.0): 1,\n",
       " ('jeep', 1.0): 1,\n",
       " ('indonesia', 1.0): 1,\n",
       " ('instamood', 1.0): 1,\n",
       " ('wanna', 1.0): 1,\n",
       " ('skype', 1.0): 1,\n",
       " ('may', 1.0): 1,\n",
       " ('nice', 1.0): 1,\n",
       " ('friendli', 1.0): 1,\n",
       " ('pretend', 1.0): 1,\n",
       " ('film', 1.0): 1,\n",
       " ('congratul', 1.0): 1,\n",
       " ('winner', 1.0): 1,\n",
       " ('cheesydelight', 1.0): 1,\n",
       " ('contest', 1.0): 1,\n",
       " ('address', 1.0): 1,\n",
       " ('guy', 1.0): 1,\n",
       " ('market', 1.0): 1,\n",
       " ('24/7', 1.0): 1,\n",
       " ('regret', 1.0): 1,\n",
       " ('14', 1.0): 1,\n",
       " ('hour', 1.0): 1,\n",
       " ('leav', 1.0): 1,\n",
       " ('without', 1.0): 1,\n",
       " ('delay', 1.0): 1,\n",
       " ('actual', 1.0): 1,\n",
       " ('easi', 1.0): 1,\n",
       " ('guess', 1.0): 1,\n",
       " ('train', 1.0): 1,\n",
       " ('wd', 1.0): 1,\n",
       " ('shift', 1.0): 1,\n",
       " ('engin', 1.0): 1,\n",
       " ('etc', 1.0): 1,\n",
       " ('sunburn', 1.0): 1,\n",
       " ('peel', 1.0): 1,\n",
       " ('blog', 1.0): 1,\n",
       " ('huge', 1.0): 1,\n",
       " ('warm', 1.0): 1,\n",
       " ('☆', 1.0): 1,\n",
       " ('complet', 1.0): 1,\n",
       " ('triangl', 1.0): 1,\n",
       " ('northern', 1.0): 1,\n",
       " ('ireland', 1.0): 1,\n",
       " ('sight', 1.0): 1,\n",
       " ('smthng', 1.0): 1,\n",
       " ('fr', 1.0): 1,\n",
       " ('hug', 1.0): 1,\n",
       " ('xoxo', 1.0): 1,\n",
       " ('uu', 1.0): 1,\n",
       " ('jaann', 1.0): 1,\n",
       " ('topnewfollow', 1.0): 1,\n",
       " ('connect', 1.0): 1,\n",
       " ('wonder', 1.0): 1,\n",
       " ('made', 1.0): 1,\n",
       " ('fluffi', 1.0): 1,\n",
       " ('insid', 1.0): 1,\n",
       " ('pirouett', 1.0): 1,\n",
       " ('moos', 1.0): 1,\n",
       " ('trip', 1.0): 1,\n",
       " ('philli', 1.0): 1,\n",
       " ('decemb', 1.0): 1,\n",
       " (\"i'd\", 1.0): 1,\n",
       " ('dude', 1.0): 1,\n",
       " ('x41', 1.0): 1,\n",
       " ('question', 1.0): 1,\n",
       " ('flaw', 1.0): 1,\n",
       " ('pain', 1.0): 1,\n",
       " ('negat', 1.0): 1,\n",
       " ('strength', 1.0): 1,\n",
       " ('went', 1.0): 1,\n",
       " ('solo', 1.0): 1,\n",
       " ('move', 1.0): 1,\n",
       " ('fav', 1.0): 1,\n",
       " ('nirvana', 1.0): 1,\n",
       " ('smell', 1.0): 1,\n",
       " ('teen', 1.0): 1,\n",
       " ('spirit', 1.0): 1,\n",
       " ('rip', 1.0): 1,\n",
       " ('ami', 1.0): 1,\n",
       " ('winehous', 1.0): 1,\n",
       " ('coupl', 1.0): 1,\n",
       " ('tomhiddleston', 1.0): 1,\n",
       " ('elizabetholsen', 1.0): 1,\n",
       " ('yaytheylookgreat', 1.0): 1,\n",
       " ('goodnight', 1.0): 1,\n",
       " ('vid', 1.0): 1,\n",
       " ('wake', 1.0): 1,\n",
       " ('gonna', 1.0): 1,\n",
       " ('shoot', 1.0): 1,\n",
       " ('itti', 1.0): 1,\n",
       " ('bitti', 1.0): 1,\n",
       " ('teeni', 1.0): 1,\n",
       " ('bikini', 1.0): 1,\n",
       " ('much', 1.0): 1,\n",
       " ('4th', 1.0): 1,\n",
       " ('togeth', 1.0): 1,\n",
       " ('end', 1.0): 1,\n",
       " ('xfile', 1.0): 1,\n",
       " ('content', 1.0): 1,\n",
       " ('rain', 1.0): 1,\n",
       " ('fabul', 1.0): 1,\n",
       " ('fantast', 1.0): 1,\n",
       " ('♡', 1.0): 1,\n",
       " ('jb', 1.0): 1,\n",
       " ('forev', 1.0): 1,\n",
       " ('belieb', 1.0): 1,\n",
       " ('nighti', 1.0): 1,\n",
       " ('bug', 1.0): 1,\n",
       " ('bite', 1.0): 1,\n",
       " ('bracelet', 1.0): 1,\n",
       " ('idea', 1.0): 1,\n",
       " ('foundri', 1.0): 1,\n",
       " ('game', 1.0): 1,\n",
       " ('sens', 1.0): 1,\n",
       " ('pic', 1.0): 1,\n",
       " ('ef', 1.0): 1,\n",
       " ('phone', 1.0): 1,\n",
       " ('woot', 1.0): 1,\n",
       " ('derek', 1.0): 1,\n",
       " ('use', 1.0): 1,\n",
       " ('parkshar', 1.0): 1,\n",
       " ('gloucestershir', 1.0): 1,\n",
       " ('aaaahhh', 1.0): 1,\n",
       " ('man', 1.0): 1,\n",
       " ('traffic', 1.0): 1,\n",
       " ('stress', 1.0): 1,\n",
       " ('reliev', 1.0): 1,\n",
       " (\"how'r\", 1.0): 1,\n",
       " ('arbeloa', 1.0): 1,\n",
       " ('turn', 1.0): 1,\n",
       " ('17', 1.0): 1,\n",
       " ('omg', 1.0): 1,\n",
       " ('say', 1.0): 1,\n",
       " ('europ', 1.0): 1,\n",
       " ('rise', 1.0): 1,\n",
       " ('find', 1.0): 1,\n",
       " ('hard', 1.0): 1,\n",
       " ('believ', 1.0): 1,\n",
       " ('uncount', 1.0): 1,\n",
       " ('coz', 1.0): 1,\n",
       " ('unlimit', 1.0): 1,\n",
       " ('cours', 1.0): 1,\n",
       " ('teamposit', 1.0): 1,\n",
       " ('aldub', 1.0): 1,\n",
       " ('☕', 1.0): 1,\n",
       " ('rita', 1.0): 1,\n",
       " ('info', 1.0): 1,\n",
       " (\"we'd\", 1.0): 1,\n",
       " ('way', 1.0): 1,\n",
       " ('boy', 1.0): 1,\n",
       " ('x40', 1.0): 1,\n",
       " ('true', 1.0): 1,\n",
       " ('sethi', 1.0): 1,\n",
       " ('high', 1.0): 1,\n",
       " ('exe', 1.0): 1,\n",
       " ('skeem', 1.0): 1,\n",
       " ('saam', 1.0): 1,\n",
       " ('peopl', 1.0): 1,\n",
       " ('polit', 1.0): 1,\n",
       " ('izzat', 1.0): 1,\n",
       " ('wese', 1.0): 1,\n",
       " ('trust', 1.0): 1,\n",
       " ('khawateen', 1.0): 1,\n",
       " ('k', 1.0): 1,\n",
       " ('sath', 1.0): 1,\n",
       " ('mana', 1.0): 1,\n",
       " ('kar', 1.0): 1,\n",
       " ('deya', 1.0): 1,\n",
       " ('sort', 1.0): 1,\n",
       " ('smart', 1.0): 1,\n",
       " ('hair', 1.0): 1,\n",
       " ('tbh', 1.0): 1,\n",
       " ('jacob', 1.0): 1,\n",
       " ('g', 1.0): 1,\n",
       " ('upgrad', 1.0): 1,\n",
       " ('tee', 1.0): 1,\n",
       " ('famili', 1.0): 1,\n",
       " ('person', 1.0): 1,\n",
       " ('two', 1.0): 1,\n",
       " ('convers', 1.0): 1,\n",
       " ('onlin', 1.0): 1,\n",
       " ('mclaren', 1.0): 1,\n",
       " ('fridayfeel', 1.0): 1,\n",
       " ('tgif', 1.0): 1,\n",
       " ('squar', 1.0): 1,\n",
       " ('enix', 1.0): 1,\n",
       " ('bissmillah', 1.0): 1,\n",
       " ('ya', 1.0): 1,\n",
       " ('allah', 1.0): 1,\n",
       " (\"we'r\", 1.0): 1,\n",
       " ('socent', 1.0): 1,\n",
       " ('startup', 1.0): 1,\n",
       " ('drop', 1.0): 1,\n",
       " ('your', 1.0): 1,\n",
       " ('arnd', 1.0): 1,\n",
       " ('town', 1.0): 1,\n",
       " ('basic', 1.0): 1,\n",
       " ('piss', 1.0): 1,\n",
       " ('cup', 1.0): 1,\n",
       " ('also', 1.0): 1,\n",
       " ('terribl', 1.0): 1,\n",
       " ('complic', 1.0): 1,\n",
       " ('discuss', 1.0): 1,\n",
       " ('snapchat', 1.0): 1,\n",
       " ('lynettelow', 1.0): 1,\n",
       " ('kikmenow', 1.0): 1,\n",
       " ('snapm', 1.0): 1,\n",
       " ('hot', 1.0): 1,\n",
       " ('amazon', 1.0): 1,\n",
       " ('kikmeguy', 1.0): 1,\n",
       " ('defin', 1.0): 1,\n",
       " ('grow', 1.0): 1,\n",
       " ('sport', 1.0): 1,\n",
       " ('rt', 1.0): 1,\n",
       " ('rakyat', 1.0): 1,\n",
       " ('write', 1.0): 1,\n",
       " ('sinc', 1.0): 1,\n",
       " ('mention', 1.0): 1,\n",
       " ('fli', 1.0): 1,\n",
       " ('fish', 1.0): 1,\n",
       " ('promot', 1.0): 1,\n",
       " ('post', 1.0): 1,\n",
       " ('cyber', 1.0): 1,\n",
       " ('ourdaughtersourprid', 1.0): 1,\n",
       " ('mypapamyprid', 1.0): 1,\n",
       " ('papa', 1.0): 1,\n",
       " ('coach', 1.0): 1,\n",
       " ('posit', 1.0): 1,\n",
       " ('kha', 1.0): 1,\n",
       " ('atleast', 1.0): 1,\n",
       " ('x39', 1.0): 1,\n",
       " ('mango', 1.0): 1,\n",
       " (\"lassi'\", 1.0): 1,\n",
       " (\"monty'\", 1.0): 1,\n",
       " ('marvel', 1.0): 1,\n",
       " ('though', 1.0): 1,\n",
       " ('suspect', 1.0): 1,\n",
       " ('meant', 1.0): 1,\n",
       " ('24', 1.0): 1,\n",
       " ('hr', 1.0): 1,\n",
       " ('touch', 1.0): 1,\n",
       " ('kepler', 1.0): 1,\n",
       " ('452b', 1.0): 1,\n",
       " ('chalna', 1.0): 1,\n",
       " ('hai', 1.0): 1,\n",
       " ('thankyou', 1.0): 1,\n",
       " ('hazel', 1.0): 1,\n",
       " ('food', 1.0): 1,\n",
       " ('brooklyn', 1.0): 1,\n",
       " ('pta', 1.0): 1,\n",
       " ('awak', 1.0): 1,\n",
       " ('okayi', 1.0): 1,\n",
       " ('awww', 1.0): 1,\n",
       " ('ha', 1.0): 1,\n",
       " ('doc', 1.0): 1,\n",
       " ('splendid', 1.0): 1,\n",
       " ('spam', 1.0): 1,\n",
       " ('folder', 1.0): 1,\n",
       " ('amount', 1.0): 1,\n",
       " ('nigeria', 1.0): 1,\n",
       " ('claim', 1.0): 1,\n",
       " ('rted', 1.0): 1,\n",
       " ('leg', 1.0): 1,\n",
       " ('hurt', 1.0): 1,\n",
       " ('bad', 1.0): 1,\n",
       " ('mine', 1.0): 1,\n",
       " ('saturday', 1.0): 1,\n",
       " ('thaaank', 1.0): 1,\n",
       " ('puhon', 1.0): 1,\n",
       " ('happinesss', 1.0): 1,\n",
       " ('tnc', 1.0): 1,\n",
       " ('prior', 1.0): 1,\n",
       " ('notif', 1.0): 1,\n",
       " ('probabl', 1.0): 1,\n",
       " ('funni', 1.0): 1,\n",
       " ('2:22', 1.0): 1,\n",
       " ('fat', 1.0): 1,\n",
       " ('co', 1.0): 1,\n",
       " ('ate', 1.0): 1,\n",
       " ('yuna', 1.0): 1,\n",
       " ('tamesid', 1.0): 1,\n",
       " ('´', 1.0): 1,\n",
       " ('googl', 1.0): 1,\n",
       " ('account', 1.0): 1,\n",
       " ('scouser', 1.0): 1,\n",
       " ('everyth', 1.0): 1,\n",
       " ('zoe', 1.0): 1,\n",
       " ('mate', 1.0): 1,\n",
       " ('liter', 1.0): 1,\n",
       " (\"they'r\", 1.0): 1,\n",
       " ('samee', 1.0): 1,\n",
       " ('edgar', 1.0): 1,\n",
       " ('updat', 1.0): 1,\n",
       " ('log', 1.0): 1,\n",
       " ('bring', 1.0): 1,\n",
       " ('abe', 1.0): 1,\n",
       " ('meet', 1.0): 1,\n",
       " ('x38', 1.0): 1,\n",
       " ('sigh', 1.0): 1,\n",
       " ('dreamili', 1.0): 1,\n",
       " ('pout', 1.0): 1,\n",
       " ('eye', 1.0): 1,\n",
       " ('quacketyquack', 1.0): 1,\n",
       " ('happen', 1.0): 1,\n",
       " ('phil', 1.0): 1,\n",
       " ('em', 1.0): 1,\n",
       " ('del', 1.0): 1,\n",
       " ('rodder', 1.0): 1,\n",
       " ('els', 1.0): 1,\n",
       " ('play', 1.0): 1,\n",
       " ('newest', 1.0): 1,\n",
       " ('gamejam', 1.0): 1,\n",
       " ('irish', 1.0): 1,\n",
       " ('literatur', 1.0): 1,\n",
       " ('inaccess', 1.0): 1,\n",
       " (\"kareena'\", 1.0): 1,\n",
       " ('fan', 1.0): 1,\n",
       " ('brain', 1.0): 1,\n",
       " ('dot', 1.0): 1,\n",
       " ('braindot', 1.0): 1,\n",
       " ('fair', 1.0): 1,\n",
       " ('rush', 1.0): 1,\n",
       " ('either', 1.0): 1,\n",
       " ('brandi', 1.0): 1,\n",
       " ('18', 1.0): 1,\n",
       " ('carniv', 1.0): 1,\n",
       " ('men', 1.0): 1,\n",
       " ('put', 1.0): 1,\n",
       " ('mask', 1.0): 1,\n",
       " ('xavier', 1.0): 1,\n",
       " ('forneret', 1.0): 1,\n",
       " ('jennif', 1.0): 1,\n",
       " ('site', 1.0): 1,\n",
       " ('free', 1.0): 1,\n",
       " ('50.000', 1.0): 1,\n",
       " ('8', 1.0): 1,\n",
       " ('ball', 1.0): 1,\n",
       " ('pool', 1.0): 1,\n",
       " ('coin', 1.0): 1,\n",
       " ('edit', 1.0): 1,\n",
       " ('trish', 1.0): 1,\n",
       " ('♥', 1.0): 1,\n",
       " ('grate', 1.0): 1,\n",
       " ('three', 1.0): 1,\n",
       " ('comment', 1.0): 1,\n",
       " ('wakeup', 1.0): 1,\n",
       " ('besid', 1.0): 1,\n",
       " ('dirti', 1.0): 1,\n",
       " ('sex', 1.0): 1,\n",
       " ('lmaooo', 1.0): 1,\n",
       " ('😤', 1.0): 1,\n",
       " ('loui', 1.0): 1,\n",
       " (\"he'\", 1.0): 1,\n",
       " ('throw', 1.0): 1,\n",
       " ('caus', 1.0): 1,\n",
       " ('inspir', 1.0): 1,\n",
       " ('ff', 1.0): 1,\n",
       " ('twoof', 1.0): 1,\n",
       " ('gr8', 1.0): 1,\n",
       " ('wkend', 1.0): 1,\n",
       " ('kind', 1.0): 1,\n",
       " ('exhaust', 1.0): 1,\n",
       " ('word', 1.0): 1,\n",
       " ('cheltenham', 1.0): 1,\n",
       " ('area', 1.0): 1,\n",
       " ('9', 1.0): 1,\n",
       " ('kale', 1.0): 1,\n",
       " ('crisp', 1.0): 1,\n",
       " ('ruin', 1.0): 1,\n",
       " ('x37', 1.0): 1,\n",
       " ('open', 1.0): 1,\n",
       " ('worldwid', 1.0): 1,\n",
       " ('outta', 1.0): 1,\n",
       " ('sfvbeta', 1.0): 1,\n",
       " ('vantast', 1.0): 1,\n",
       " ('xcylin', 1.0): 1,\n",
       " ('bundl', 1.0): 1,\n",
       " ('show', 1.0): 1,\n",
       " ('internet', 1.0): 1,\n",
       " ('price', 1.0): 1,\n",
       " ('realisticli', 1.0): 1,\n",
       " ('pay', 1.0): 1,\n",
       " ('net', 1.0): 1,\n",
       " ('educ', 1.0): 1,\n",
       " ('power', 1.0): 1,\n",
       " ('weapon', 1.0): 1,\n",
       " ('nelson', 1.0): 1,\n",
       " ('mandela', 1.0): 1,\n",
       " ('recent', 1.0): 1,\n",
       " ('j', 1.0): 1,\n",
       " ('chenab', 1.0): 1,\n",
       " ('flow', 1.0): 1,\n",
       " ('pakistan', 1.0): 1,\n",
       " ('incredibleindia', 1.0): 1,\n",
       " ('teenchoic', 1.0): 1,\n",
       " ('choiceinternationalartist', 1.0): 1,\n",
       " ('superjunior', 1.0): 1,\n",
       " ('caught', 1.0): 1,\n",
       " ('first', 1.0): 1,\n",
       " ('salmon', 1.0): 1,\n",
       " ('super-blend', 1.0): 1,\n",
       " ('project', 1.0): 1,\n",
       " ('youth@bipolaruk.org.uk', 1.0): 1,\n",
       " ('awesom', 1.0): 1,\n",
       " ('stream', 1.0): 1,\n",
       " ('artist', 1.0): 1,\n",
       " ('alma', 1.0): 1,\n",
       " ('mater', 1.0): 1,\n",
       " ('highschoolday', 1.0): 1,\n",
       " ('clientvisit', 1.0): 1,\n",
       " ('faith', 1.0): 1,\n",
       " ('christian', 1.0): 1,\n",
       " ('school', 1.0): 1,\n",
       " ('lizaminnelli', 1.0): 1,\n",
       " ('upcom', 1.0): 1,\n",
       " ('uk', 1.0): 1,\n",
       " ('😄', 1.0): 1,\n",
       " ('singl', 1.0): 1,\n",
       " ('hill', 1.0): 1,\n",
       " ('everi', 1.0): 1,\n",
       " ('beat', 1.0): 1,\n",
       " ('wrong', 1.0): 1,\n",
       " ('readi', 1.0): 1,\n",
       " ('natur', 1.0): 1,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = build_freqs(train_x, train_y)\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba653098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 7. 5.]]\n"
     ]
    }
   ],
   "source": [
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67ddcec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.69314718.\n",
      "The resulting vector of weights is [0.0, 0.0, -0.0]\n"
     ]
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c58b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    # extract of the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    print('x: ', x)\n",
    "    y_pred = sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    print('y_pred: ', y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1179b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [[1. 1. 1.]]\n",
      "y_pred:  [[0.5]]\n",
      "I am happy -> 0.500\n",
      "x:  [[1. 1. 1.]]\n",
      "y_pred:  [[0.5]]\n",
      "I am bad -> 0.500\n",
      "x:  [[1. 2. 2.]]\n",
      "y_pred:  [[0.5]]\n",
      "this movie should have been great. -> 0.500\n",
      "x:  [[1. 1. 1.]]\n",
      "y_pred:  [[0.5]]\n",
      "great -> 0.500\n",
      "x:  [[1. 2. 2.]]\n",
      "y_pred:  [[0.5]]\n",
      "great great -> 0.500\n",
      "x:  [[1. 3. 3.]]\n",
      "y_pred:  [[0.5]]\n",
      "great great great -> 0.500\n",
      "x:  [[1. 4. 4.]]\n",
      "y_pred:  [[0.5]]\n",
      "great great great great -> 0.500\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( '%s -> %.3f' % (tweet, predict_tweet(tweet, freqs, theta)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b136ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d5cad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
